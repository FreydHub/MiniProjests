{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbb93457",
   "metadata": {},
   "source": [
    "## Методические указания по выполнению лабораторной работы №3\n",
    "\n",
    "**Тема: Обнаружение объектов с использованием Faster R-CNN**\n",
    "\n",
    "**Цель работы:** Ознакомиться с архитектурой Faster R-CNN и принципами двухэтапного обнаружения объектов.\n",
    "\n",
    "**Задачи:**\n",
    "- Изучить теоретические основы двухэтапного обнаружения объектов: роль RPN и классификационного этапа в Faster R-CNN.\n",
    "- Загрузить предобученную модель Faster R-CNN.\n",
    "- Ознакомиться с форматом аннотаций для обучения в задаче детекции.\n",
    "- Визуализировать предсказания, проанализировать ошибки модели и провести исследование по поиску баланса FN/FP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9931204b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 1. Теоретическая часть\n",
    "\n",
    "В данной лабораторной работе мы познакомимся с задачей детекции на примере архитектуры [Faster R-CNN](https://arxiv.org/pdf/1506.01497), обученной на наборе данных [COCO](https://cocodataset.org/#home), а также с новым форматом данных для обучения нейро-сетевых моделей детекции. Для оценки модели воспользуемся набором данных [Pascal VOC 2007](http://host.robots.ox.ac.uk/pascal/VOC/).\n",
    "\n",
    "**Перед тем, как приступать к выполнению практической части, ознакомьтесь с первоисточниками используемых компонентов, а также документацией по [ссылке](https://pytorch.org/vision/master/models/faster_rcnn.html), включающей подробности работы с моделью и новым форматом данных, сэмплы кода.**\n",
    "\n",
    "#### 1.1 Архитектура Faster R-CNN\n",
    "\n",
    "Существует два основных подхода к обнаружению объектов:\n",
    "Двухстадийные модели – более точные, но медленные.\n",
    "Одностадийные модели – быстрые, но менее точные.\n",
    "\n",
    "Faster R-CNN — это двухэтапная модель детекции объектов. В отличие от ResNeXt, Faster R-CNN не просто классифицирует изображение, а находит на нём несколько объектов, предсказывает bounding boxes и присваивает метки классам. Она состоит из следующих компонентов:\n",
    "\n",
    "1. Backbone (ResNet/VGG/MobileNet) – извлекает признаки из изображения.\n",
    "2. Region Proposal Network (RPN) – предлагает области, где могут находиться объекты.\n",
    "3. ROI Pooling + Fully Connected Layers – классифицирует объекты и уточняет bounding boxes.\n",
    "4. Non-Maximum Suppression (NMS) – убирает дублирующиеся предсказания.\n",
    "\n",
    "\n",
    "В предыдущих лабораторных работах мы познакомились с классификацией изображений, где модель предсказывает единственный класс для всего изображения. Однако во многих задачах компьютерного зрения классификация недостаточна. Например, когда на одном изображении присутствуют несколько объектов разных классов необходимо не только определить, что изображено, но и где это находится.\n",
    "\n",
    "Faster R-CNN –  модель, которая также решает задачу обнаружения объектов, добавляя к классификации локализацию. \n",
    "\n",
    "#### 1.2 Формат данных для задачи детекции\n",
    "\n",
    "Faster R-CNN требует разметки изображений, помимо классов включающей в себя и координаты bounding box по оси x, y.\n",
    "Ознакомьтесь с форматом набора данных Pascal VOC, скачайте аннотации набора данных и изучите структуру **.xml** файлов в папке Annotations по [ссылке](http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtestnoimgs_06-Nov-2007.tar).\n",
    "\n",
    "#### 1.3 Оценка качества\n",
    "\n",
    "Помимо известных вам инструментов оценить качество работы детектора могут помочь:\n",
    "\n",
    "Confidence Score - значение, указывающее на уверенность модели в том, что на данном месте изображения находится объект. Модель предсказывает этот параметр для каждого предсказанного bounding box. Чем выше confidence score, тем более уверена модель в своём предсказании.\n",
    "\n",
    "Фильтрация предсказаний - модель может предсказать много объектов, но не все из них будут точными. Чтобы уменьшить количество ложных срабатываний (False Positives), применяется фильтр предсказаний, используя confidence threshold. Если confidence score меньше заданного порога - предсказание отбрасывается.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f91f75c",
   "metadata": {},
   "source": [
    "### 2. Практическая часть\n",
    "\n",
    "#### 2.1 Подготовка окружения\n",
    "\n",
    "Установите зависимости и библиотеки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6441a4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b8ad59",
   "metadata": {},
   "source": [
    "#### 2.2 Подготовка модели\n",
    "\n",
    "Загрузите предобученную модель, определите устройство, переведите модель в режим инференса:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad5ffb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-3): 4 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузка предобученной модели Faster R-CNN\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2bcad1",
   "metadata": {},
   "source": [
    "#### 2.3 Загрузка и предобработка изображений\n",
    "\n",
    "\n",
    "Затем импортируйте датасет из соответствующих пакетов PyTorch и определите метод трансформации данных для подачи в модель. Он понадобится позже для преобразования изображений при прямом проходе через модель чтобы получить предсказания:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1ae375c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пути к датасету PASCAL VOC 2007\n",
    "voc_images_path = \"JPEGImages\"\n",
    "voc_annotations_path = \"Annotations\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd27810",
   "metadata": {},
   "source": [
    "#### 2.4 Объявление методов для работы с данными\n",
    "\n",
    "Далее необходимо создать методы препроцессинга: метод чтения файла аннотации для возврата numpy-объекта содержащего bounding boxes, и метод отрисовки истиных и прогнозных bounding boxes на изображении для визуализации полученных результатов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "399b7250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для обработки аннотаций PASCAL VOC\n",
    "def parse_voc_annotation(xml_path):\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "    boxes = []\n",
    "    labels = []\n",
    "    \n",
    "    for obj in root.findall(\"object\"):\n",
    "        label = obj.find(\"name\").text\n",
    "        bbox = obj.find(\"bndbox\")\n",
    "        xmin, ymin, xmax, ymax = map(int, [\n",
    "            bbox.find(\"xmin\").text,\n",
    "            bbox.find(\"ymin\").text,\n",
    "            bbox.find(\"xmax\").text,\n",
    "            bbox.find(\"ymax\").text\n",
    "        ])\n",
    "        boxes.append([xmin, ymin, xmax, ymax])\n",
    "        labels.append(label)\n",
    "    \n",
    "    return np.array(boxes), labels\n",
    "\n",
    "# Функция для визуализации предсказаний\n",
    "def draw_boxes(image, boxes, labels, color=\"blue\"):\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    for box, label in zip(boxes, labels):\n",
    "        xmin, ymin, xmax, ymax = map(int, box)\n",
    "        draw.rectangle([xmin, ymin, xmax, ymax], outline=color, width=2)\n",
    "        draw.text((xmin, ymin), f\"{label}\", fill=color)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b60f68a",
   "metadata": {},
   "source": [
    "#### 2.5 Анализ False Positives / False Negatives\n",
    "\n",
    "Для измерения того, насколько хорошо bounding box предсказан, применяется параметр IoU (Intersection over Union) между двумя bounding boxes. Чем он выше, тем точнее предсказание. Для его оценки опишем следующий метод, принимающий два np.array-объекта (прогнозные и истинные координаты):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9de59854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для вычисления IoU\n",
    "def compute_iou(box1, box2):\n",
    "    x1 = max(box1[0], box2[0])\n",
    "    y1 = max(box1[1], box2[1])\n",
    "    x2 = min(box1[2], box2[2])\n",
    "    y2 = min(box1[3], box2[3])\n",
    "    inter_area = max(0, x2 - x1) * max(0, y2 - y1)\n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    iou = inter_area / (box1_area + box2_area - inter_area)\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fccf59",
   "metadata": {},
   "source": [
    "#### 2.6 Оценка модели и визуализация результатов\n",
    "\n",
    "Выполните прямой проход нескольких изображений через модель. Для этого необходимо загрузить изображения и аннотации, применить преобразование изображений в тензор. Затем получите выходы модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a39e4c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.6 Оценка модели и визуализация результатов\n",
    "N = 5  # Количество изображений для оценки\n",
    "image_filenames = os.listdir(voc_images_path)[:N]\n",
    "\n",
    "false_positives = 0\n",
    "false_negatives = 0\n",
    "\n",
    "for image_name in image_filenames:\n",
    "    image_path = os.path.join(voc_images_path, image_name)\n",
    "    annotation_path = os.path.join(voc_annotations_path, image_name.replace(\".jpg\", \".xml\"))\n",
    "    \n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    true_boxes, true_labels = parse_voc_annotation(annotation_path)\n",
    "    \n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    image_tensor = transform(image).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(image_tensor)\n",
    "    \n",
    "    pred_boxes = outputs[0]['boxes'].cpu().numpy()\n",
    "    pred_labels = outputs[0]['labels'].cpu().numpy()\n",
    "    pred_scores = outputs[0]['scores'].cpu().numpy()\n",
    "    \n",
    "    confidence_threshold = 0.5\n",
    "    valid_indices = np.where(pred_scores > confidence_threshold)[0]\n",
    "    pred_boxes = pred_boxes[valid_indices]\n",
    "    pred_labels = pred_labels[valid_indices]\n",
    "    pred_scores = pred_scores[valid_indices]\n",
    "    \n",
    "    image_with_preds = draw_boxes(image.copy(), pred_boxes, pred_labels, color=\"red\")\n",
    "    image_with_truth = draw_boxes(image.copy(), true_boxes, true_labels, color=\"green\")\n",
    "    \n",
    "    image_with_preds.show(title=\"Predictions\")\n",
    "    image_with_truth.show(title=\"Ground Truth\")\n",
    "\n",
    "    # Вычисление IoU и подсчет FN/FP\n",
    "    matched = np.zeros(len(true_boxes))\n",
    "    for pred_box in pred_boxes:\n",
    "        ious = [compute_iou(pred_box, true_box) for true_box in true_boxes]\n",
    "        if max(ious) > 0.5:\n",
    "            matched[np.argmax(ious)] = 1\n",
    "        else:\n",
    "            false_positives += 1\n",
    "    \n",
    "    false_negatives += len(true_boxes) - sum(matched)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b119dba",
   "metadata": {},
   "source": [
    "#### 2.7 Поиск оптимальной конфигурации \n",
    "\n",
    "Проанализируйте полученные результаты с выбранным значением фильтрации предсказаний. Проведите исследование с целью поиска оптимального порога и баланса FN/FP. Обоснуйте полученные результаты:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb58b167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Positives: 10, False Negatives: 1.0\n",
      "FP Rate: 0.91, FN Rate: 0.09\n",
      "Оптимальный порог: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_22120\\2331076962.py:27: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  fp_rate = false_positives / (false_positives + false_negatives)\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_22120\\2331076962.py:28: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  fn_rate = false_negatives / (false_positives + false_negatives)\n"
     ]
    }
   ],
   "source": [
    "# 2.7 Поиск оптимальной конфигурации\n",
    "fp_rate = false_positives / (false_positives + false_negatives)\n",
    "fn_rate = false_negatives / (false_positives + false_negatives)\n",
    "print(f\"False Positives: {false_positives}, False Negatives: {false_negatives}\")\n",
    "print(f\"FP Rate: {fp_rate:.2f}, FN Rate: {fn_rate:.2f}\")\n",
    "\n",
    "# Поиск оптимального порога\n",
    "best_threshold = 0.5\n",
    "best_balance = abs(fp_rate - fn_rate)\n",
    "for threshold in np.arange(0.3, 0.9, 0.05):\n",
    "    valid_indices = np.where(pred_scores > threshold)[0]\n",
    "    pred_boxes = pred_boxes[valid_indices]\n",
    "    pred_labels = pred_labels[valid_indices]\n",
    "    pred_scores = pred_scores[valid_indices]\n",
    "    \n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    matched = np.zeros(len(true_boxes))\n",
    "    for pred_box in pred_boxes:\n",
    "        ious = [compute_iou(pred_box, true_box) for true_box in true_boxes]\n",
    "        if max(ious) > 0.5:\n",
    "            matched[np.argmax(ious)] = 1\n",
    "        else:\n",
    "            false_positives += 1\n",
    "    \n",
    "    false_negatives += len(true_boxes) - sum(matched)\n",
    "    fp_rate = false_positives / (false_positives + false_negatives)\n",
    "    fn_rate = false_negatives / (false_positives + false_negatives)\n",
    "    balance = abs(fp_rate - fn_rate)\n",
    "    \n",
    "    if balance < best_balance:\n",
    "        best_balance = balance\n",
    "        best_threshold = threshold\n",
    "\n",
    "print(f\"Оптимальный порог: {best_threshold}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d083f17-b1e5-4848-b707-306dce117434",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7074c0a9-97d3-44c3-af45-54f87db0acfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e95af2-603e-49f5-952e-8f8e8efaa3f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
